{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning models for GTSRB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, LeakyReLU \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "\n",
    "# to display confusion matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, os.path.sep)\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] == classNames\n",
    "\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_png(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [32,32])\n",
    "\n",
    "def get_bytes_and_label(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label\n",
    "\n",
    "def show_data(s1,l1, s2,l2, labels, min):\n",
    "    fig, ax = plt.subplots()\n",
    "    X = np.arange(len(s1))\n",
    "\n",
    "    models = labels\n",
    "    plt.bar(X, s1, width = 0.4, color = 'b', label=l1)\n",
    "    plt.bar(X + 0.4, s2, color = 'r', width = 0.4, label = l2)\n",
    "    plt.xticks(X + 0.4 / 2, models)\n",
    "    plt.ylim(top = 100, bottom = min)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def show_batch(image_batch, label_batch):\n",
    "  columns = 8\n",
    "  rows = BATCH_SIZE / columns + 1  \n",
    "  plt.figure(figsize=(10, 2 * rows))\n",
    "  for n in range(BATCH_SIZE):\n",
    "      ax = plt.subplot(int(rows), columns, n+1)\n",
    "      plt.imshow((image_batch[n]))\n",
    "      plt.title(classNames[label_batch[n]==1][0])\n",
    "      plt.axis('off')\n",
    "\n",
    "\n",
    "def show_history(history):\n",
    "    print(history.history.keys())\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='lower right')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "def show_accuracies(labels, test, val): \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    X = np.arange(len(test))\n",
    "\n",
    "    plt.bar(X, test, width = 0.4, color = 'b', label='test')\n",
    "    plt.bar(X + 0.4, val, color = 'r', width = 0.4, label = \"val\")\n",
    "    plt.xticks(X + 0.4 / 2, labels)\n",
    "    plt.ylim(top = 1.0, bottom = 0.97)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "def show_misclassified(predictions, ground_truth, images, num_rows = 5, num_cols=3):\n",
    "    \n",
    "    # Plot the first X test images with wrong predictions.\n",
    "    num_images = num_rows*num_cols\n",
    "    print(num_images)\n",
    "    plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "    i = 0\n",
    "    k = 0\n",
    "    while k < len(images) and i < num_images:\n",
    "        predicted_label = np.argmax(predictions[k])\n",
    "        gt = np.where(ground_truth[k])[0][0]\n",
    "        if predicted_label != gt:\n",
    "            plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "            plot_image(k, predictions[k], gt, images)\n",
    "            plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "            plot_value_array(k, predictions[k], ground_truth)\n",
    "            i += 1\n",
    "        k += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array, true_label, img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    \n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "      color = 'blue'\n",
    "    else:\n",
    "      color = 'red'\n",
    "    \n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(classNames[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                classNames[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array, true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(8))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(8), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[np.where(true_label)[0][0]].set_color('blue')   \n",
    "\n",
    "\n",
    "\n",
    "def show_confusion_matrix(mat, classes):\n",
    "\n",
    "    df_cm = pd.DataFrame(mat, range(classes), range(classes))\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sn.set(font_scale=1.4) # for label size\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='d') # font size\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size\n",
    "\n",
    "* [1]  https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Default values\n",
    "\n",
    "Subject to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "N_CLASSES = 43\n",
    "N_CHANNELS = 3\n",
    "KERNEL_SIZE = (5,5)\n",
    "N_EPOCHS = 25\n",
    "\n",
    "TRAIN_ONLINE = True\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'c:/Users/Utilizador/Documents/GitHub/VCPI-Grupo/Datasets/train_images'\n",
    "test_path = 'c:/Users/Utilizador/Documents/GitHub/VCPI-Grupo/Datasets/test_images'\n",
    "logs_path = 'c:/Users/Utilizador/Documents/GitHub/VCPI-Grupo/Logs'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_callbacks(file_path):\n",
    "    \n",
    "    pointer = ModelCheckpoint(filepath=file_path, monitor='val_accuracy', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "    stopper = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=15, verbose=1)\n",
    "    reduceLr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0)\n",
    "    \n",
    "    return [pointer, stopper, reduceLr]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare to Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00000', '00001', '00002', '00003', '00004', '00005', '00006',\n",
       "       '00007', '00008', '00009', '00010', '00011', '00012', '00013',\n",
       "       '00014', '00015', '00016', '00017', '00018', '00019', '00020',\n",
       "       '00021', '00022', '00023', '00024', '00025', '00026', '00027',\n",
       "       '00028', '00029', '00030', '00031', '00032', '00033', '00034',\n",
       "       '00035', '00036', '00037', '00038', '00039', '00040', '00041',\n",
       "       '00042', 'GTSRB'], dtype='<U5')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = np.array(os.listdir(f'{data_path}'))\n",
    "\n",
    "    \n",
    "class_names # Better format this output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_png(data):\n",
    "    for i in range (N_CLASSES):\n",
    "\n",
    "        path = os.path.join(data, format(i, '05d'))\n",
    "        files = os.listdir(path)\n",
    "        csv = \"\"\n",
    "\n",
    "        for file in files:\n",
    "            try:\n",
    "                image = Image.open(os.path.join(path,file))\n",
    "                image.save(f\"{path}/{file.split('.')[0]}.png\")\n",
    "            except:\n",
    "                pass\n",
    "    print(f'Finished converting all files in {data} into png files.') \n",
    "            \n",
    "    # data = tf.data.Dataset.list_files(f\"{data}/*/*.png\", shuffle=False)\n",
    "    # image_count = len(data) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished converting all files in c:/Users/Utilizador/Documents/GitHub/VCPI-Grupo/Datasets/train_images to png files.\n",
      "Finished converting all files in c:/Users/Utilizador/Documents/GitHub/VCPI-Grupo/Datasets/test_images to png files.\n"
     ]
    }
   ],
   "source": [
    "convert_to_png(data_path)\n",
    "\n",
    "convert_to_png(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39209 files belonging to 44 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    f'{data_path}',\n",
    "    label_mode='categorical',\n",
    "    image_size=(32,32),\n",
    "    shuffle=True\n",
    ") # This might be wrong: if I put batch_size it breaks\n",
    "\n",
    "normalize = tf.keras.layers.experimental.preprocessing.Rescaling(1.0 / 255)\n",
    "dataset = dataset.map(lambda x, y: (normalize(x), y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-cuda8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
